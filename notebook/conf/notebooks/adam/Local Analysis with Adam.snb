{
  "metadata" : {
    "name" : "Local Analysis with Adam",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":local-repo /tmp/spark-notebook/repo",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res2: String = Repo changed to /tmp/spark-notebook/repo!\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "Repo changed to /tmp/spark-notebook/repo!"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":dp org.bdgenomics.adam % adam-apis % 0.16.0\n- org.apache.hadoop % hadoop-client %   _\n- org.apache.spark  % spark-core    %   _\n- org.scala-lang    %     _         %   _\n- org.scoverage     %     _         %   _\n+ org.apache.spark  %  spark-mllib_2.10  % 1.2.1",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "warning: there were 2 feature warning(s); re-run with -feature for details\njars: Array[String] = [Ljava.lang.String;@7bd34b7d\nres6: List[String] = List(/tmp/spark-notebook/repo/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar, /tmp/spark-notebook/repo/com/fasterxml/jackson/core/jackson-annotations/2.1.1/jackson-annotations-2.1.1.jar, /tmp/spark-notebook/repo/com/twitter/parquet-common/1.6.0rc4/parquet-common-1.6.0rc4.jar, /tmp/spark-notebook/repo/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar, /tmp/spark-notebook/repo/org/apache/ant/ant-launcher/1.8.2/ant-launcher-1.8.2.jar, /tmp/spark-notebook/repo/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar, /tmp/spark-notebook/repo/commons-net/commons-net/2.2/commons-net-2.2.jar, /tmp/spark-notebook/repo/org/json4s/json4s-jackson_2.10/3.2.10/json4s-jackson_2.10-3.2.10.jar, /tmp/spark-notebook/repo/org/j..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<table><tr><td>/tmp/spark-notebook/repo/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar</td></tr><tr><td>/tmp/spark-notebook/repo/com/fasterxml/jackson/core/jackson-annotations/2.1.1/jackson-annotations-2.1.1.jar</td></tr><tr><td>/tmp/spark-notebook/repo/com/twitter/parquet-common/1.6.0rc4/parquet-common-1.6.0rc4.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/apache/ant/ant-launcher/1.8.2/ant-launcher-1.8.2.jar</td></tr><tr><td>/tmp/spark-notebook/repo/net/jpountz/lz4/lz4/1.2.0/lz4-1.2.0.jar</td></tr><tr><td>/tmp/spark-notebook/repo/commons-net/commons-net/2.2/commons-net-2.2.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/json4s/json4s-jackson_2.10/3.2.10/json4s-jackson_2.10-3.2.10.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/json4s/json4s-ast_2.10/3.2.10/json4s-ast_2.10-3.2.10.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/spark-project/pyrolite/2.0.1/pyrolite-2.0.1.jar</td></tr><tr><td>/tmp/spark-notebook/repo/com/twitter/chill-java/0.5.0/chill-java-0.5.0.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar</td></tr><tr><td>/tmp/spark-notebook/repo/com/twitter/parquet-avro/1.6.0rc4/parquet-avro-1.6.0rc4.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/eclipse/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/tachyonproject/tachyon-client/0.5.0/tachyon-client-0.5.0.jar</td></tr><tr><td>/tmp/spark-notebook/repo/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar</td></tr><tr><td>/tmp/spark-notebook/repo/commons-cli/commons-cli/1.2/commons-cli-1.2.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/apache/httpcomponents/httpclient/4.3.2/httpclient-4.3.2.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/bdgenomics/adam/adam-core/0.16.0/adam-core-0.16.0.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar</td></tr><tr><td>/tmp/spark-notebook/repo/org/slf4j/jcl-over-slf4j/1.7.5/jcl-over-slf4j-1.7.5.jar</td></tr><tr><td>/tmp/spark-notebook/repo/com/amazonaws/aws-java-sdk/1.7.5/aws-java-sdk-1.7.5.jar</td></tr><tr><td>/tmp/spark-notebook/repo/io/netty/netty/3.8.0.Final/netty-3.8.0.Final.jar</td></tr><tr><td>...</td></tr></table>"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "reset(lastChanges = _.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n                     .set(\"spark.kryo.registrator\", \"org.bdgenomics.adam.serialization.ADAMKryoRegistrator\")\n                     .set(\"spark.kryoserializer.buffer.mb\", \"4\")\n                     .set(\"spark.kryo.referenceTracking\", \"true\")\n                     .setAppName(\"ADAM 1000genomes\")\n                     .set(\"spark.executor.memory\", \"3g\")\n)",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.hadoop.fs.{FileSystem, Path}\n//import org.bdgenomics.adam.converters.{ VCFLine, VCFLineConverter, VCFLineParser }\nimport org.bdgenomics.formats.avro.{Genotype, FlatGenotype}\nimport org.bdgenomics.adam.models.VariantContext\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.rdd.ADAMContext\nimport org.apache.spark.rdd.RDD",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.hadoop.fs.{FileSystem, Path}\nimport org.bdgenomics.formats.avro.{Genotype, FlatGenotype}\nimport org.bdgenomics.adam.models.VariantContext\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.rdd.ADAMContext\nimport org.apache.spark.rdd.RDD\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import sys.process._\nval vcf = \"/tmp/6-sample.vcf\"\nif (!new java.io.File(vcf).exists) {\n  new java.net.URL(\"http://med-at-scale.s3.amazonaws.com/samples/6-sample.vcf\")  #> new java.io.File(vcf) !!\n}\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "warning: there were 1 feature warning(s); re-run with -feature for details\nimport sys.process._\nvcf: String = /tmp/6-sample.vcf\nres8: Any = \"\"\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val gts: RDD[Genotype] = sparkContext.loadGenotypes(vcf)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "gts: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Genotype] = FlatMappedRDD[2] at flatMap at ADAMContext.scala:464\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "FlatMappedRDD[2] at flatMap at ADAMContext.scala:464"
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val localAdam = vcf+\"6\"+\".adam\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "localAdam: String = /tmp/6-sample.vcf6.adam\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/tmp/6-sample.vcf6.adam"
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.bdgenomics.adam.rdd.ADAMContext._\ngts.adamParquetSave(localAdam)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.bdgenomics.adam.rdd.ADAMContext._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val gts2:RDD[Genotype] = sparkContext.loadGenotypes(localAdam)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "gts2: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Genotype] = MappedRDD[5] at map at ADAMContext.scala:172\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MappedRDD[5] at map at ADAMContext.scala:172"
      },
      "output_type" : "execute_result",
      "execution_count" : 10
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "gts2.first",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 3, localhost): java.lang.ArrayStoreException: org.apache.avro.generic.GenericData$Record\n\tat scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)\n\tat scala.Array$.slowcopy(Array.scala:81)\n\tat scala.Array$.copy(Array.scala:107)\n\tat scala.collection.mutable.ResizableArray$class.copyToArray(ResizableArray.scala:77)\n\tat scala.collection.mutable.ArrayBuffer.copyToArray(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.copyToArray(TraversableOnce.scala:241)\n\tat scala.collection.AbstractTraversable.copyToArray(Traversable.scala:105)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:249)\n\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:105)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$27.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDD$$anonfun$27.apply(RDD.scala:1098)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// populations to select\nval pops = Set(\"GBR\", \"ASW\", \"CHB\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "pops: scala.collection.immutable.Set[String] = Set(GBR, ASW, CHB)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "Set(GBR, ASW, CHB)"
      },
      "output_type" : "execute_result",
      "execution_count" : 12
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val panelFile = \"/tmp/ALL.panel\"\ns\"wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel -O ${panelFile}\"!!",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "warning: there were 1 feature warning(s); re-run with -feature for details\npanelFile: String = /tmp/ALL.panel\nres11: String = \"\"\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 13
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import scala.io.Source\ndef extract(filter: (String, String) => Boolean= (s, t) => true) = Source.fromFile(panelFile).getLines().map( line => {\n  val toks = line.split(\"\\t\").toList\n  toks(0) -> toks(1)\n}).toMap.filter( tup => filter(tup._1, tup._2) )\n  \n// panel extract from file, filtering by the 2 populations\ndef panel: Map[String,String] = \n  extract((sampleID: String, pop: String) => pops.contains(pop)) \n  \n// broadcast the panel \nval bPanel = sparkContext.broadcast(panel)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import scala.io.Source\nextract: (filter: (String, String) => Boolean)scala.collection.immutable.Map[String,String]\npanel: Map[String,String]\nbPanel: org.apache.spark.broadcast.Broadcast[Map[String,String]] = Broadcast(4)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "Broadcast(4)"
      },
      "output_type" : "execute_result",
      "execution_count" : 14
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val finalGts = gts2.filter(g =>  bPanel.value.contains(g.getSampleId))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "finalGts: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Genotype] = FilteredRDD[6] at filter at <console>:62\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "FilteredRDD[6] at filter at &lt;console&gt;:62"
      },
      "output_type" : "execute_result",
      "execution_count" : 15
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sampleCount = finalGts.map(_.getSampleId.toString.hashCode).distinct.count\ns\"#Samples: $sampleCount\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 5, localhost): java.lang.ClassCastException: org.apache.avro.generic.GenericData$Record cannot be cast to org.bdgenomics.formats.avro.Genotype\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:62)\n\tat scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:56)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:220)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//utils\nimport scala.collection.JavaConverters._\nimport org.bdgenomics.formats.avro._\ndef variantId(g:Genotype):String = {\n  val name = g.getVariant.getContig.getContigName\n    val start = g.getVariant.getStart\n    val end = g.getVariant.getEnd\n    s\"$name:$start:$end\"\n}\ndef asDouble(g:Genotype):Double = g.getAlleles.asScala.count(_ != GenotypeAllele.Ref)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import scala.collection.JavaConverters._\nimport org.bdgenomics.formats.avro._\nvariantId: (g: org.bdgenomics.formats.avro.Genotype)String\nasDouble: (g: org.bdgenomics.formats.avro.Genotype)Double\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 17
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.SparkContext._\n// A VARIANT SHOULD HAVE sampleCount GENOTYPES\nval variantsById = finalGts.keyBy(g => variantId(g).hashCode).groupByKey.cache\nval missingVariantsRDD = variantsById.filter { case (k, it) => it.size != sampleCount }.keys\n\n// IF SAVING LIST OF MISSING VARIANTS IS REQUIRED...\n//missingVariantsRDD.saveAsObjectFile(hu(\"/model/missing-variants\"))\n\n// could be broadcased as well...\nval missingVariants = missingVariantsRDD.collect().toSet",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:75: error: not found: value sampleCount\n       val missingVariantsRDD = variantsById.filter { case (k, it) => it.size != sampleCount }.keys\n                                                                                 ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val completeGts = finalGts.filter { g => ! (missingVariants contains variantId(g).hashCode) }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:72: error: not found: value missingVariants\n       val completeGts = finalGts.filter { g => ! (missingVariants contains variantId(g).hashCode) }\n                                                   ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sampleToData: RDD[(String, (Double, Int))] =\n    completeGts.map { g => (g.getSampleId.toString, (asDouble(g), variantId(g).hashCode)) }\n\nval groupedSampleToData = sampleToData.groupByKey",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:59: error: value groupByKey is not a member of org.apache.spark.rdd.RDD[(String, (Double, Int))]\n       val groupedSampleToData = sampleToData.groupByKey\n                                              ^\n<console>:57: error: not found: value completeGts\n           completeGts.map { g => (g.getSampleId.toString, (asDouble(g), variantId(g).hashCode)) }\n           ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.linalg.{Vector=>MLVector, Vectors}\ndef makeSortedVector(gts: Iterable[(Double, Int)]): MLVector = Vectors.dense( gts.toArray.sortBy(_._2).map(_._1) )\n\nval dataPerSampleId:RDD[(String, MLVector)] =\n    groupedSampleToData.mapValues { it =>\n        makeSortedVector(it)\n    }\n\nval dataFrame:RDD[MLVector] = dataPerSampleId.values",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:60: error: not found: value groupedSampleToData\n           groupedSampleToData.mapValues { it =>\n           ^\n<console>:64: error: value values is not a member of org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)]\n       val dataFrame:RDD[MLVector] = dataPerSampleId.values\n                                                     ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.clustering.{KMeans,KMeansModel}\nval model: KMeansModel = KMeans.train(dataFrame, 3, 10)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:53: error: not found: value dataFrame\n       val model: KMeansModel = KMeans.train(dataFrame, 3, 10)\n                                             ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val predictions: RDD[(String, (Int, String))] = dataPerSampleId.map(elt => {\n    (elt._1, ( model.predict(elt._2), bPanel.value.getOrElse(elt._1, \"\"))) \n})",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:62: error: not found: value dataPerSampleId\n       val predictions: RDD[(String, (Int, String))] = dataPerSampleId.map(elt => {\n                                                       ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val confMat = predictions.collect.toMap.values\n    .groupBy(_._2).mapValues(_.map(_._1))\n    .mapValues(xs => (1 to 3).map( i => xs.count(_ == i-1)).toList)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:52: error: not found: value predictions\n       val confMat = predictions.collect.toMap.values\n                     ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "<table>\n<tr><td></td><td>#0</td><td>#1</td><td>#2</td></tr>\n{ for (popu <- confMat) yield\n  <tr><td>{popu._1}</td> { for (cnt <- popu._2) yield \n    <td>{cnt}</td>\n  }\n  </tr>\n}\n</table>",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:55: error: not found: value confMat\n{ for (popu <- confMat) yield\n               ^\n"
    } ]
  } ],
  "nbformat" : 4
}