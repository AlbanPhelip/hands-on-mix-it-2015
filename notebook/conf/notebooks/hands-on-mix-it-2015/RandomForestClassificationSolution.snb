{
  "metadata" : {
    "name" : "RandomForestClassificationSolution",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Random Forest Classification"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###By default a SparkContext is available in the variable ***sparkContext***. We put it in a variable named ***sc*** for more simplicity"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sc = sparkContext",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@c7206a0\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.SparkContext@c7206a0"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###We import all the libraries we will need in this notebook"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###We declare several functions that will be used just after"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The featureEngineering function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def featureEngineering(data : RDD[String]): RDD[LabeledPoint] = {\n\n  data.map(line => {\n\n    val values = line.replaceAll(\"\\\"\",\"\").split('$')\n\n    val label = values(1).toDouble\n\n    val pClass = values(0).toDouble\n    val sex = values(3) match {\n      case \"male\" => 0d\n      case \"female\" => 1d\n    }\n    val age = values(4) match {\n      case \"NA\" => 28d\n      case l => l.toDouble\n    }\n    val sibsp = values(5).toDouble\n    val parch = values(6).toDouble\n    val fair = values(8) match {\n      case \"NA\" => 14.45\n      case l => l.toDouble\n    }\n    val embarked = values(10) match {\n      case \"\" => 0d\n      case \"C\" => 1d\n      case \"Q\" => 2d\n      case \"S\" => 3d\n    }\n\n    val cleanedData = Array(pClass, sex, age, sibsp, parch, fair, embarked)\n\n    LabeledPoint(label, Vectors.dense(cleanedData))\n  })\n\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "featureEngineering: (data: org.apache.spark.rdd.RDD[String])org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The extract header function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n * Extract header of a dataset\n * @param rdd A RDD with a header inside\n * @return A tuple2. First element of the tuple is the header. Second element is the data.\n */\ndef extractHeader(rdd: RDD[String]): (String, RDD[String]) = {\n\n  // Take the first line (csv schema)\n  val schema = rdd.first()\n\n  // Remove first line from first partition only\n  (schema, rdd.mapPartitionsWithIndex {\n    case (0, l) => l.drop(1)\n    case (_, l) => l\n  })\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "extractHeader: (rdd: org.apache.spark.rdd.RDD[String])(String, org.apache.spark.rdd.RDD[String])\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The getMetricsRandomForest function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n *\n * @param model A RandomForestModel from the method RandomForest.trainClassifier()\n * @param data the data (a RDD[LabeledPoint])\n * @return A tuple giving the accuracy and the confusion matrix\n */\ndef getMetricsRandomForest(model: RandomForestModel, data: RDD[LabeledPoint]): (Double, Matrix) = {\n\n  val predictionsAndLabels = data.map(l => (model.predict(l.features), l.label))\n\n  val metrics: MulticlassMetrics = new MulticlassMetrics(predictionsAndLabels)\n\n  val accuracy = 100d * metrics.precision\n  val confusion = metrics.confusionMatrix\n\n  (accuracy, confusion)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "getMetricsRandomForest: (model: org.apache.spark.mllib.tree.model.RandomForestModel, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])(Double, org.apache.spark.mllib.linalg.Matrix)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The randomForestTrainClassifier function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n * Train a Random Forest Classifier\n * @param data: RDD[LabeledPoint] - The training set\n * @param categoricalFeaturesInfo: A Map indicating which features are categorical and how many categories they contain\n * @param numTrees: The number of trees to train\n * @param featuresSubsetStrategy: Strategy to select a subset of the features for splitting (use \"auto\")\n * @param impurity: The impurity measure to select the best feature for splitting (\"entropy\" or \"gini\")\n * @param maxDepth: The maximum depth of each tree\n * @param maxBins: The maximum number of leaves for each tree\n * @return A RandomForestClassifier Model, usable to predict new data\n */\ndef randomForestTrainClassifier(categoricalFeaturesInfo: Map[Int, Int] = Map[Int, Int](),\n                               numTrees: Int = 10,\n                               featuresSubsetStrategy: String = \"auto\",\n                               impurity: String = \"entropy\",\n                               maxDepth: Int = 2,\n                               maxBins: Int = 12)(data: RDD[LabeledPoint]) : RandomForestModel = {\n  RandomForest.trainClassifier(data, 2, categoricalFeaturesInfo, numTrees, featuresSubsetStrategy, impurity, maxDepth, maxBins)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "randomForestTrainClassifier: (categoricalFeaturesInfo: Map[Int,Int], numTrees: Int, featuresSubsetStrategy: String, impurity: String, maxDepth: Int, maxBins: Int)(data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])org.apache.spark.mllib.tree.model.RandomForestModel\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###Your turn now ! Just follow the instructions"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Loading data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : read file ./data/data_titanic.csv\nval rawData = sc.textFile(\"./data/data_titanic.csv\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.rdd.RDD[String] = ./data/data_titanic.csv MapPartitionsRDD[1] at textFile at <console>:44\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "./data/data_titanic.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:44"
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Parsing Data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : use extracHeader method to get an RDD without header\nval data = extractHeader(rawData)._2",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at mapPartitionsWithIndex at <console>:52\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[2] at mapPartitionsWithIndex at &lt;console&gt;:52"
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Feature Engineering"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : use the featureEngineering method to get the cleaned data.\n// Be carefull, you will get a RDD[LabeledPoint]\nval cleanData = featureEngineering(data)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "cleanData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[3] at map at <console>:43\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[3] at map at &lt;console&gt;:43"
      },
      "output_type" : "execute_result",
      "execution_count" : 10
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Splitting Data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : split the cleaned data in a train and test set (proportions 0.75, 0.25) using the 'randomSplit' method\nval Array(trainSet, testSet) = cleanData.randomSplit(Array(0.75, 0.25))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "trainSet: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = PartitionwiseSampledRDD[268] at randomSplit at <console>:58\ntestSet: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = PartitionwiseSampledRDD[269] at randomSplit at <console>:58\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "PartitionwiseSampledRDD[269] at randomSplit at &lt;console&gt;:58"
      },
      "output_type" : "execute_result",
      "execution_count" : 25
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Modelling"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#####Tuning Parameters"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val categoricalFeaturesInfo = Map(1 -> 2, 6 -> 4)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map(1 -> 2, 6 -> 4)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"container-fluid\"><div><div class=\"col-md-12\"><div>\n    <script data-this=\"{&quot;dataId&quot;:&quot;anondd8d120ccf242d9e7d51c8a874fd5f4e&quot;,&quot;dataInit&quot;:[{&quot;Value&quot;:&quot;Value&quot;,&quot;key&quot;:1,&quot;values&quot;:&quot;List(1, 2)&quot;,&quot;headers&quot;:&quot;List(Key, Value)&quot;,&quot;value&quot;:2,&quot;Key&quot;:&quot;Key&quot;},{&quot;Value&quot;:&quot;Value&quot;,&quot;key&quot;:6,&quot;values&quot;:&quot;List(6, 4)&quot;,&quot;headers&quot;:&quot;List(Key, Value)&quot;,&quot;value&quot;:4,&quot;Key&quot;:&quot;Key&quot;}],&quot;genId&quot;:&quot;1792636663&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"key\",\"value\",\"Key\",\"Value\",\"headers\",\"values\"],\"nrow\":2,\"shown\":25,\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script></div></div></div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 12
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#####Training the model"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : use the decisionTreeTrainClassifier method to train a Decision Tree (Use the parameters of your choice)\nval model = randomForestTrainClassifier(categoricalFeaturesInfo = categoricalFeaturesInfo, numTrees = 50,\n      impurity = \"entropy\", maxDepth = 10, maxBins = 50)(trainSet)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "model: org.apache.spark.mllib.tree.model.RandomForestModel = \nTreeEnsembleModel classifier with 50 trees\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "TreeEnsembleModel classifier with 50 trees\n"
      },
      "output_type" : "execute_result",
      "execution_count" : 26
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Prediction and Evaluation"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : get the accuracy and confusion matrix for the prediction on the test set using the getMetricsDecisionTree method\n// TODO : do the same for the train set for comparison\nval (accuracyTrain, confusionTrain) = getMetricsRandomForest(model, trainSet)\nval (accuracyTest, confusionTest) = getMetricsRandomForest(model, testSet)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "accuracyTrain: Double = 90.54187192118228\nconfusionTrain: org.apache.spark.mllib.linalg.Matrix = \n609.0  19.0   \n77.0   310.0  \naccuracyTest: Double = 80.95238095238095\nconfusionTest: org.apache.spark.mllib.linalg.Matrix = \n158.0  23.0  \n33.0   80.0  \n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "158.0  23.0  \n33.0   80.0  "
      },
      "output_type" : "execute_result",
      "execution_count" : 27
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Print the results"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "println(s\"Results for the training set\")\nprintln(s\"\\t Accuracy: $accuracyTrain %\")\nprintln(s\"\\t Confusion Matrix: \\n $confusionTrain\")\n\nprintln(s\"Results for the test set\")\nprintln(s\"\\t Accuracy: $accuracyTest %\")\nprintln(s\"\\t Confusion Matrix: \\n $confusionTest\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Results for the training set\n\t Accuracy: 89.2518440463646 %\n\t Confusion Matrix: \n 551.0  31.0   \n71.0   296.0  \nResults for the test set\n\t Accuracy: 78.88888888888889 %\n\t Confusion Matrix: \n 196.0  31.0  \n45.0   88.0  \n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 15
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}