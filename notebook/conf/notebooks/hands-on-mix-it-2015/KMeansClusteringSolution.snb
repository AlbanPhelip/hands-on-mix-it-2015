{
  "metadata" : {
    "name" : "KMeansClusteringSolution",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# KMeans Clustering"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###By default a SparkContext is available in the variable ***sparkContext***. We put it in a variable named ***sc*** for more simplicity"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sc = sparkContext",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@36556e80\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.SparkContext@36556e80"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###We import all the libraries we will need in this notebook"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.clustering.KMeans\nimport org.apache.spark.mllib.clustering.KMeansModel\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.clustering.KMeans\nimport org.apache.spark.mllib.clustering.KMeansModel\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###We declare several functions that will be used just after"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The featureEngineering function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def featureEngineering(data : RDD[String]): RDD[LabeledPoint] = {\n  data.map(line => {\n\n    val values = line.replaceAll(\"\\\"\",\"\").split('$')\n    val label = values(1).toDouble\n\n    val pClass = values(0).toDouble\n\n    val age = values(4) match {\n      case \"NA\" => 28d\n      case l => l.toDouble\n    }\n    val sibsp = values(5).toDouble\n    val parch = values(6).toDouble\n    val fair = values(8) match {\n      case \"NA\" => 14.45\n      case l => l.toDouble\n    }\n\n    val numericalData = Array(pClass, age, sibsp, parch, fair)\n\n    LabeledPoint(label, Vectors.dense(numericalData))\n  })\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "featureEngineering: (data: org.apache.spark.rdd.RDD[String])org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The extract header function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n * Extract header of a dataset\n * @param rdd A RDD with a header inside\n * @return A tuple2. First element of the tuple is the header. Second element is the data.\n */\ndef extractHeader(rdd: RDD[String]): (String, RDD[String]) = {\n  // Take the first line (csv schema)\n  val schema = rdd.first()\n\n  // Remove first line from first partition only\n    (schema, rdd.mapPartitionsWithIndex {\n      case (0, l) => l.drop(1)\n      case (_, l) => l\n    })\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "extractHeader: (rdd: org.apache.spark.rdd.RDD[String])(String, org.apache.spark.rdd.RDD[String])\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####The getStatsPerCluster function"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n   * Gives the centroids of a KmeansModel and the proportion of survivors epr cluster\n   * @param model A KMeansModel from the method Kmeans.train()\n   * @param data The data (a RDD[LabeledPoint])\n   * @return The centroids and the proportion of survivors\n   */\n  def getStatsPerCluster(model: KMeansModel, data: RDD[LabeledPoint]) = {\n    val predictionAndLabels = data.map(l => (model.predict(l.features), l.label))\n    val centroids = model.clusterCenters\n\n    val numberOfDeathPerCluster = predictionAndLabels.reduceByKey(_+_).sortByKey()\n    val totalCountPerCluster = predictionAndLabels.map(l => (l._1, 1)).reduceByKey(_+_).sortByKey()\n\n    val proportionOfDeathPerCluster = numberOfDeathPerCluster.zip(totalCountPerCluster).map(l => (l._1._1, l._1._2/l._2._2))\n    proportionOfDeathPerCluster.map(l => (centroids(l._1), l._2))\n  }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "getStatsPerCluster: (model: org.apache.spark.mllib.clustering.KMeansModel, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, Double)]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "###Your turn now ! Just follow the instructions"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "####Loading data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : read file ./data/data_titanic.csv\nval rawData = sc.textFile(\"./data/data_titanic.csv\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rawData: org.apache.spark.rdd.RDD[String] = ./data/data_titanic.csv MapPartitionsRDD[1] at textFile at <console>:44\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "./data/data_titanic.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:44"
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Parsing Data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : use extracHeader method to get an RDD without header\nval data = extractHeader(rawData)._2",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at mapPartitionsWithIndex at <console>:51\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[2] at mapPartitionsWithIndex at &lt;console&gt;:51"
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Feature Engineering"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : use the featureEngineering method to get the cleaned data.\n// Be carefull, you will get a RDD[LabeledPoint]\nval cleanData = featureEngineering(data)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "cleanData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[3] at map at <console>:42\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[3] at map at &lt;console&gt;:42"
      },
      "output_type" : "execute_result",
      "execution_count" : 9
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Get the features"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : get the data we want to analyse (RDD[Vector])\nval featuredData = cleanData.map(_.features)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "featuredData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[4] at map at <console>:54\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[4] at map at &lt;console&gt;:54"
      },
      "output_type" : "execute_result",
      "execution_count" : 10
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Modelling"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : Train a KMeans model on the data set\nval model = KMeans.train(featuredData, 4, 20)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "model: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@66ddf8ba\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.mllib.clustering.KMeansModel@66ddf8ba"
      },
      "output_type" : "execute_result",
      "execution_count" : 11
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Inspect population of each cluster"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// TODO : For each cluster look the center and the percentage of survivor\n// Use the getStatsPerCluster method\nval statsPerCluster = getStatsPerCluster(model, cleanData)\nstatsPerCluster.foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "([1.0,41.0,0.0,0.5,512.3292],1.0)\n([1.0,35.5304238095238,0.6984126984126984,0.873015873015873,195.38710317460314],0.6825396825396826)\n([2.5499524262607043,28.148192197906756,0.38154138915318747,0.3082778306374881,14.937745004757385],0.3235014272121789)\n([1.3455497382198953,34.7303664921466,1.0890052356020943,0.6439790575916231,70.71551099476441],0.5916230366492147)\nstatsPerCluster: org.apache.spark.rdd.RDD[(org.apache.spark.mllib.linalg.Vector, Double)] = MapPartitionsRDD[52] at map at <console>:55\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 12
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}