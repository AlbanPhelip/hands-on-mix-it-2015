{
  "metadata" : {
    "name" : "Spark on Mesos using Cassandra",
    "user_save_timestamp" : "2014-12-03T22:12:58.863Z",
    "auto_save_timestamp" : "2014-12-03T22:10:42.476Z",
    "language_info" : {
      "name" : "Scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Spark config"
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":local-repo /tmp/repo",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":dp com.datastax.spark % spark-cassandra-connector_2.10 % 1.1.0",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sparkAssembly = \"hdfs://nnode:8020/spark-1.1.0.tgz\"\nval sparkLocalDir = \"/mnt4/spark-local\"\nval executorMem = \"5G\"\n// faced some problems with Torrent...\nval broadcastFactory = \"org.apache.spark.broadcast.HttpBroadcastFactory\"\nval mesosDriverNode = \"localhost\"\nval zks:List[(String, Int)] = List((\"zk0\", 2181), (\"zk0\", 2181))\n\nval cassandraHost = \"10.97.2.104\"\n  \nreset(\"Notebook on dnode-3\", lastChanges = (c:SparkConf) => {\n  c.set(\"spark.executor.memory\", executorMem)\n   .set(\"spark.executor.uri\", sparkAssembly)\n   .set(\"spark.cassandra.connection.host\", cassandraHost)\n   .set(\"spark.master\", zks.map{case (h,p)=>s\"$h:$p\"}.mkString(\"mesos://zk://\", \",\", \"/mesos\"))\n   .set(\"spark.driver.host\", mesosDriverNode)\n   .set(\"spark.broadcast.factory\", broadcastFactory)\n   .set(\"spark.local.dir\", sparkLocalDir)\n   //.set(\"spark.storage.memoryFraction\", \"0.1\")\n   //.set(\"spark.shuffle.memoryFraction\", \"0.4\")\n})",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "sparkContext.getConf.toDebugString",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.ui.notebook.front.widgets.SparkInfo\nimport scala.concurrent.duration._\nnew SparkInfo(sparkContext, checkInterval=1 second, execNumber=None)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Access cassandra"
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import com.datastax.spark.connector._                                    \nimport org.apache.spark.sql.cassandra.CassandraSQLContext\n\nval cc = new CassandraSQLContext(sparkContext)\nval rdd: org.apache.spark.sql.SchemaRDD = cc.sql(\"SELECT * from tch.actualvalues\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "rdd.count",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import cc._\n\nval proj = rdd.select( 'id, 'metrics )",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import SparkContext._\nval l = proj.map(r => (r.getString(0), r(1).asInstanceOf[Map[String, Double]].get(\"mem\")))\n    .filter(_._2.isDefined)\n    .map(x => (x._1, x._2.get))\n    .reduceByKey(_ + _)\n    .take(100).toList\n<ul>{for ((x,y) <- l) yield <li>{x} â†’ {y}</li>}</ul>  ",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":sql select * from test where name = {String: name} and first = {String: first}",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}